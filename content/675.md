Title: When memory matters: float64 vs float32 for storing data
Date: 2013-12-16 15:49
Author: nipunbatra
Tags: Machine Learning, Python
Slug: when-memory-matters
Category: Blog

Dealing with large dataset can be a bit of bother. Hardware has grown,
but so have the datasets. Recently, while loading some Electricity
datasets into memory with Pandas and processing on them, I was running
out of memory.

[Jack][] suggested that I use float32 instead of the default float64
provided by Pandas. Following is a comparison of the in-memory
comparisons between the two datatypes for representing the same data.

Firstly, as suggested by Jeff on [Stack overflow][], here is small
function to find the size requirements (in bytes) of a DataFrame

	In [2]: def sizedf(df):  
	...: return df.values.nbytes + df.index.nbytes + df.columns.nbytes  


Let us create a large dataframe.


	In [3]: a =pd.DataFrame(random.randn(100000,10))

	In [4]: a  
	Out[4]:  
	<class 'pandas.core.frame.DataFrame'> 
	Int64Index: 100000 entries, 0 to 99999  
	Data columns (total 10 columns):  
	0 100000 non-null values  
	1 100000 non-null values  
	2 100000 non-null values  
	3 100000 non-null values  
	4 100000 non-null values  
	5 100000 non-null values  
	6 100000 non-null values  
	7 100000 non-null values  
	8 100000 non-null values  
	9 100000 non-null values  
	dtypes: float64(10)


We can see that the data type of the dataframe is float64. Let us now
create a dataframe b, casted as float32.

	In [5]: b = a.astype(float32)

	In [6]: b  
	Out[6]:  
	\<class 'pandas.core.frame.DataFrame'\>  
	Int64Index: 100000 entries, 0 to 99999  
	Data columns (total 10 columns):  
	0 100000 non-null values  
	1 100000 non-null values  
	2 100000 non-null values  
	3 100000 non-null values  
	4 100000 non-null values  
	5 100000 non-null values  
	6 100000 non-null values  
	7 100000 non-null values  
	8 100000 non-null values  
	9 100000 non-null values  
	dtypes: float32(10)  

Now, let us compare the in-memory sizes of the two dataframes.

	In [7]: size_df(a)  
	Out[7]: 8800080

	In [8]: size_df(b)  
	Out[8]: 4800080  

The dataframe with datatype as float32 does a lot better. What about
memory usage on disk?  
Let us export the data to HDF5 store and see.

	In [9]: a.to_hdf('df_a.h5', 'df')

	In [10]: b.to_hdf('df_b.h5', 'df')

	In [11]: ! ls -lah df_a.h5  
	-rw-rw-r-- 1 nipun nipun 8.4M Dec 16 15:22 df_a.h5

	In [12]: ! ls -lah df_b.h5  
	-rw-rw-r-- 1 nipun nipun 4.6M Dec 16 15:42 df_b.h5  

Clearly, we save a lot of on disk and raw memory when using float32 as
opposed to float64(which is the default). Whenever, we don't need such
high precision, float32 might just save you a lot of memory issues!

  [Jack]: http://jack-kelly.com/
  [Stack overflow]: http://stackoverflow.com/questions/18089667/pandas-how-to-estimate-how-much-memory-a-dataframe-will-need
